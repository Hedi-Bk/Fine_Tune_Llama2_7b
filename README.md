# Fine-Tuning LLaMA-2-7B with QLoRA

Fine-tuning the **LLaMA-2-7B** model using **QLoRA** on a dataset of **1000 examples**, fully executed on **Google Colab**.

---

## Google Colab Notebook

Click here to open the project:

[** Open in Google Colab**](https://colab.research.google.com/drive/1vn2CqUE9M8StV9fk_IcjTArGwPvSYotB)

---

## Project Objective

- Fine-tune a **LLaMA-2-7B** model
- Use **QLoRA (Quantized Low-Rank Adaptation)**
- Train **only LoRA adapters**
- Load the base model in **4-bit NF4** using **bitsandbytes**
- Environment: **Google Colab**

---

## Technologies Used

- **LLaMA-2-7B**
- **QLoRA (HuggingFace PEFT)**
- **BitsAndBytes (4-bit NF4 quantization)**
- **Google Colab**
- **Hugging Face Transformers**

---

## Main Resource Used

This project is heavily inspired by the tutorial from:

**Krish Naik (YouTube)**

Tutorial: _Fine-tuning LLaMA-2 using QLoRA_

---

## Note

> This project was entirely developed on Google Colab (no local code).
>
> The GitHub repository is used only for documentation and project organization.

---

## License

Free to use for educational purposes.
